# Specification: LocalAI Hot Enabling at Runtime

## Overview

This track enhances the LocalAI integration to support enabling and disabling the service dynamically at runtime via the existing UI toggle in the Admin Dashboard. It also extends the functionality to control the LocalAI container/process lifecycle, supports runtime model selection, and ensures that segment summaries are automatically routed to LocalAI when it is active.

## Functional Requirements

- **Runtime Toggling**: The system must monitor the state of the LocalAI "Enabled" setting and update the service registry (e.g., `SegmentNarrationServiceFactory`) immediately without requiring a restart.
- **Validation Health Check**:
  - When an administrator toggles LocalAI to "Enabled", the system must immediately perform a health check (ping) to the configured LocalAI URL.
  - The UI must show a loading state or non-blocking notification while the check is in progress.
- **Fail-Safe Enablement**:
  - If the health check fails, the enablement must be aborted, the toggle reverted, and an error shown.
- **Dynamic Service Availability**: Once enabled, LocalAI must be immediately available for narration and image generation.
- **Process Lifecycle Management**:
  - Provide UI controls to start/stop the LocalAI process/container (e.g., Docker).
  - Display current process status (Running, Stopped, etc.).
- **Runtime Model Selection**: Allow changing the LocalAI model (e.g., `llama-3`) via the settings UI with immediate effect.
- **Automatic Summary Handover**:
  - When LocalAI is enabled and active, the `SegmentSummaryService` must prioritize it for generating segment summaries.

## Acceptance Criteria

- [ ] Admin can toggle LocalAI "On"; it becomes usable if reachable, else reverts.
- [ ] Admin can start/stop the LocalAI service via the UI.
- [ ] Admin can change the LocalAI model at runtime.
- [ ] Segment summaries are generated by LocalAI when it is enabled.

## Out of Scope

- None.

